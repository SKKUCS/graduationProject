# 중간보고서

## Abstract

 본 연구의 목적은 강화학습 기법을 사용해 임의의 게임 환경에서 목표를 완수할 수 있는 구현체를 제작하는 것이다. 구현의 토대는 구글사의 기계학습 프레임워크 텐서플로우를 사용했으며, 사용한 언어는 파이썬이다. 강화학습 알고리즘으로는 심층 큐 신경망과 비동기적 우위 행동-평가 알고리즘을 선택하였다. 기존의 구현은 대체로 하나의 게임을 분석해 에이전트와 행동반경, 그리고 장애물 등을 분류하여 최적 해를 구하는 방식이었으나, 본 연구에서는 게임 환경 자체를 정지된 화면의 연속으로 파악해 일련의 상황에 대한 보상을 계산하고 해당 장면으로 넘어갈 수 있는 동작을 수행하도록 구현하였다. 이를 통해 기존의 게임 플레이 에이전트가 갖고 있던 도메인의 한계를 극복하고, 화면의 비트맵만 전달할 수 있다면 어떠한 종류의 게임도 학습할 수 있는 일반 알고리즘을 구현하고자 하였다.

## 서론

### 제안배경 및 필요성

 “인공지능은 새 시대의 전기(電氣)다.” 앤드류 응 박사의 말처럼, 인공지능은 시대의 화두가 되었다. 현재 2.0을 맞이하며 가장 널리 쓰이고 있는 기계학습 프레임워크 텐서플로우는 4,100만회 이상 다운로드 되었으며, 최근 2019년 3월 6일과 7일에 걸쳐 개최된 Tensorflow Dev Summit에는 천명 이상의 전문가와 애호가, 그리고 수만명 이상의 중계 시청자가 참여했다. 알파고 이후 단순히 인공지능에 대한 환상 혹은 추상적인 의제설정을 넘어, 실제 모델을 구현하고 개발해 사용하는 시대가 된 것이다. 연구소, 대기업은 물론이고, 사회 초년생들의 스타트업, 심지어 중-고교에서까지 인공지능을 이야기하고, 그것을 통해 상상을 구현하고 있다.

 최근 컴퓨터의 연산력 증대와 함께 널리 쓰이고 있는 강화학습은 이런 흐름을 더욱 가속화한다. 기존의 방법론이 규칙을 설정한 이후 그 정해진 규칙을 수행하는 일에 국한되어 있었다면, 강화학습 및 딥러닝은 그 한계를 넘어 명확한 규칙이 없는 상황에서도 보상함수를 따라 기계가 학습해 결과를 낼 수 있는 구조를 만들었다. 분야의 제한 또한 거의 없어, 영상인식, 자연어 처리, 혹은 투자 등 다양한 지점에서 인공지능이 만들어지고 있다.

 많은 사례 가운데 본 연구에서 채택한 것은 게임에 적용하는 모델이다. 본디 게임이라는 것은 한정된 상태에서 한정된 선택지로 특정한 목적을 이루는 것이다. 이런 특성상 알파고 이전의 게임 인공지능들은 모든 경우의 수를 저장한 뒤, 최적 값을 빠르게 찾는 경로탐색 알고리즘에 가까웠다. 그러나 실제 게임을 해보면, 유한한 경우의 수라 해도 모든 경우의 수를 따지는 것은 사실상 불가능한 일에 가깝다. 현대시대의 게임의 경우의 수는 상상할 수 없을 만큼 많고, 이것에 대한 모든 경우의 수를 계산해 최적 해를 찾는 것은 지금의 하드웨어로도 불가능에 가까운 일이다.

 바둑의 예시가 대표적으로, 바둑판 위에서 가능한 모든 수를 계산하는 것은 불가능하다. 체스가 완벽하게 파훼된 것과 대조적으로, 이런 이유 때문에 아무리 컴퓨터가 우수해도 바둑에서 사람을 이기는 것은 불가능하다는 말이 자주 나왔다. 그러나 우리가 알파고를 통해 확인한 것은, 이런 전체 경우의 수를 계산하는 것이 아닌 인간과 유사한 방식으로 ‘학습’하는 알고리즘이 성취를 이뤄낼 수 있다는 사실이었다. 실제 알파고는 이세돌과 커제를 꺾음으로서 그 성능을 증명했고, 이후 새로운 방식으로 구현된 알파고 제로는 이 알파고와 상대로도 전승을 했다고 알려져 있다.

 이런 성취를 이뤘음에도 불구하고, 다른 분야에 비해 게임 인공지능은 낮은 수준에 머물러있다. 그 기저에는 세 가지 요소가 있는데, 하나는 기존 규칙 기반 인공지능이 이미 도입된 상태기에 바꾸기 어렵다는 점, 다른 하나는 강화학습 기반의 인공지능은 막대한 연산력이 필요해 규모가 큰 다목적 게임에서 처리하기 어렵다는 점, 그리고 마지막으로 그 잠재력을 아직 사람들이 인지하지 못한다는 점이다.

 그렇다면 게임 인공지능은 왜 필요한가? 게임의 정의를 다시 생각해보면, 게임 상황을 해결하고 목적을 달성할 수 있는 인공지능은 게임의 일반해를 구하는 과정이라 볼 수 있다. 즉, 게임 인공지능은 특정 상황과 보상함수, 그리고 환경의 정보가 입력되었을 때 하나의 해결책을 제시할 수 있고, 이것은 현재 답이 없다고 알려진 문제들이나 경우의 수를 쉽게 고려하기 어려운 문제들에 대해 하나의 지침을 제공한다. 또한 단순히 지침을 제공할 뿐만 아니라, 더 많은 정보와 시간을 주었을 때 더욱 최적 해에 가까운 답을 내놓을 수도 있다. 이런 문제들에 대한 기존 해법이 인간의 경험적 판단이었음을 미루어 볼 때, 컴퓨터의 ‘학습’을 통한 새로운 판단은 시간대비 효율이 우수할 것이라 예상해볼 수 있다.

 단순히 그 확장성뿐만 아니라, 학습하고 수행할 수 있는 게임 인공지능이 게임 산업 전반에 미칠 영향도 기대할만하다. 게임이란 장르의 핵심적 요소는 언제 어디서든 기기만 있으면 플레이할 수 있다는 점에 있다. 이때, 각자의 시공간이 다르므로 시간이 흐를수록 같이 게임을 하기는 어려워진다. 그럴 때 보통 인공지능과 상대를 하게 되는데, 기존의 인공지능들과 상대를 할 경우 몇 번의 시행 이후에는 인공지능의 패턴을 학습해, 특정 패턴으로 인공지능이 잘못된 수를 두도록 유도하거나 인공지능이 인식하지 못하는 수를 둠으로써 승리하는 ‘크랙’ 혹은 ‘치팅’ 방식이 존재했다.

 뿐만 아니라 대규모 접속 게임의 경우, 제대로 유저로서 기능을 수행하지 못하고 자원만 고갈시키는 ‘봇’의 존재로 인해 사용자 경험이 현저하게 저해되는 현상이 자주 발생한다. 정해진 규칙에 의해 프로그래밍 된 기존의 에이전트는 학습은커녕 ‘상호작용’ 자체가 불가능하고, 이런 에이전트들은 실제 사용자들에게 불쾌감과 위화감을 조성한다. 그에 비해 학습이 가능한 인공지능은 환경 내에서의 모든 요소에 대해 학습 및 반응을 할 수 있고, 추후 자연어 처리와 같은 다른 기법과 융합해 보다 실제 사용자에 가까운 모습을 연출할 수 있다.

 지금까지 연구 결과의 의의를 살펴보았다면, 이제 과정의 의의를 살펴보자. 앞서 살펴본 바와 같이 현재의 게임 인공지능은 정해진 규칙을 수행하는 형태가 많다. 본 연구의 방법론은 이런 규칙이 아닌, 단순히 진행도에 따른 보상과 페널티를 부여하는 보상함수와 화면 정보를 통한 강화학습이다. 이 방식의 특장점은 환경의 종류에 상관없이 화면 데이터를 통해 학습이 가능하다는 점이다. 기존의 방식은 대부분의 특징을 엄밀히 규정해야 했고, 그에 따른 분류 계층이 포함되었다. 반면 본 연구의 방식은 그런 분류를 하지 않는 각각의 화면 자체의 값을 평가하고, 이러한 추상화를 통해 학습된 모델을 다른 플랫폼에도 바로 적용할 수 있도록 하였다.

 이에 더하여 본 연구의 지향은 게임 내에 존재할 수 있는 다양한 객체에 인공지능을 적용하는 것으로, 실제 상용 게임 안에는 수백-수만의 객체가 존재하기 때문에 각 모델이 학습하기 위해선 연산의 최적화가 절실하다. 현재 강화학습 평가의 주된 평가는 높은 정확도에 있지만, 본 연구에서는 학습에 필요한 자원 및 시간 또한 면밀히 고려하고자 하였다. 아래에 보다 자세히 설명하겠지만, 심층 큐 신경망의 학습속도를 개선한 비동기적 우위 행동-평가 모델을 채용해 보급형 CPU로도 학습을 진행할 수 있게 되었다.

 연구의 학습 도메인은 닌텐도사의 게임 ‘슈퍼 마리오 브라더스’로, 채택 배경은 다음과 같다. 하나는 게임의 내용이 어렵지 않다는 점이다. 강화학습의 특성상 보상함수의 반환 값을 계산할 수 없으면 학습이 이루어지지 않기 때문에 적당한 난이도의 게임을 고르고자 했다. 다른 하나는 게임의 화면 데이터를 파이썬으로 내보낼 수 있는 라이브러리가 존재한다는 점이다. 상용 게임의 화면을 렌더링 이전 데이터로 출력하는 것은 리버스 엔지니어링에 가깝고, 연구의 목적과는 조금 거리가 있는 일이다. 따라서 이미 해당 내용을 진행할 수 있는 라이브러리가 있는 게임을 선택했다.

### 작품의 목표

 처음 작품의 목표는 전략 시뮬레이션 게임을 플레이하는 에이전트를 만드는 것이었다. 그렇기 때문에 고전게임 스타크래프트1에서 플레이할 수 있는 에이전트 BWAPI를 연구했다. 그러나 그 과정에서는 몇 가지 한계가 있었다. 첫째는 해당 라이브러리가 정식 인가를 받지 못한 연구용 라이브러리였다는 것이다. 연구 도중 ‘스타크래프트 리마스터’가 출시되면서, BWAPI는 코드 전반을 뜯어 고쳐야 실행할 수 있는 라이브러리가 되었고, 구버전 스타크래프트를 구하는 합법적인 방법은 찾기 어려웠다. 둘째로, 해당 라이브러리 및 게임의 구조가 도메인 한정적이었다는 점이다. 기본적으로 BWAPI는 전략의 집합을 설정하고, 그 중 가장 이길 확률이 높은 전략을 계산해서 경로를 탐색하는 방식의 인공지능이었다. 해당 방식은 일반 해를 구하고자 하는 연구의 방향성과 맞지 않는 부분이 많았다. 

 옮긴 주제는 스타크래프트2였다. 스타크래프트2는 블리자드사와 딥마인드사간의 협력으로 PySC2라는 공식 라이브러리를 제공했다. 또한 라이브러리 자체도 순수하게 에이전트의 행동만 분리해둔 계층으로, 강화학습을 시도하기에 적합한 플랫폼이었다. 그러나 연구를 진행하다보니, 현재 여건으로 시도하기에는 몇가지 어려움이 있었다. 먼저 학습 난이도가 매우 높다는 점이다. 일반적으로 사람도 어려워하는 게임인 스타크래프트2의 경우 일정한 성취를 위해 필요한 분당 행동수가 300회 이상으로 알려져 있고, 정보 또한 불완전하며 경우의 수가 무한한 환경이다. 또한 그렇기 때문에 학습을 위해서는 고성능의 컴퓨팅 자원이 필요하다. 따라서 여건상 플랫폼을 선택하기 어렵다고 판단했다.

 결국 최종적으로 정해진 연구의 학습 도메인은 닌텐도사의 게임 ‘슈퍼 마리오 브라더스’로, 채택 배경은 다음과 같다. 하나는 게임의 내용이 어렵지 않다는 점이다. 강화학습의 특성상 보상함수의 반환 값을 계산할 수 없으면 학습이 이루어지지 않기 때문에 적당한 난이도의 게임을 고르고자 했다. 다른 하나는 게임의 화면 데이터를 파이썬으로 내보낼 수 있는 라이브러리가 존재한다는 점이다. 상용 게임의 화면을 렌더링 이전 데이터로 출력하는 것은 리버스 엔지니어링에 가깝고, 연구의 목적과는 조금 거리가 있는 일이다. 따라서 이미 해당 내용을 진행할 수 있는 라이브러리가 있는 게임을 선택했다.

 작품의 기본적인 목표는 상술한 슈퍼 마리오 브라더스 게임의 스테이지 1-1을 완수하는 것이다. 적절한 난이도의 플랫포머 게임을 클리어하는 과정을 통해 인공지능은 ‘게임’에 대한 개념을 습득하고, 추후 추가적인 스테이지 클리어 및 다른 게임에도 적용하는 것을 기대해볼 수 있다. 

 보다 세부적으로 목표를 설명하자면, 에이전트와 환경 객체들의 분류를 화면 데이터의 변화에 따른 보상함수 출력값의 변경만을 통해 계산하는 것이다. 실제 슈퍼 마리오 브라더스라는 게임을 살펴보면, 다양한 문제가 있음을 확인할 수 있다. 경로를 방해하는 장애물이 있으며, 닿으면 죽는 객체도 있다. 닿으면 죽는 객체 중에서도 몬스터가 있고, 흔히 탄막(Sprite)이라 불리는 파티클도 있다. 본 연구에서는 이런 정보가 에이전트에게 따로 주어지지 않고, 오직 화면과 결과값의 보상 합계를 통해 에이전트의 행동이 결정된다.

 이 방식을 채택한 이유는 ‘과연 사람은 어떻게 게임을 하는가?’ 라는 질문에 대한 대답이기 때문이다. 실제 게임을 플레이할 때, 프로그래머 혹은 게임업계 종사자가 아닌 이상 객체 각각을 분석하고 경우의 수를 계산하는 사람은 거의 없다. 실제 사람이 게임을 할 때는 게임 화면만이 주어지는 정보로, 그 화면 자체의 변화를 보고 자신의 게임 결과를 판단하다. 즉, 장애물이 있기 때문에 지나가지 못하는 것이 아니라, 지나가지 못하기 때문에 장애물임을 알게 되는 것이다. 따라서 우리의 목표는 이렇게 인간의 학습방식과 가깝게 화면 정보 전체만을 통해서 게임을 클리어할 수 있는 에이전트를 만드는 것이다.

### 작품 전체 Overview

 기본적으로, 작품의 구조는 다음과 같다.

 (다이어그램 삽입)

 번호와 흐름 순으로 하나씩 짚어보겠다.

 1.
 2.
 3.
 4. ...

## 관련 연구

 Deepmind DQN, A3C 원본 페이퍼
 슈퍼마리오 관련 시도된 알고리즘들, 프로젝트들 나열


## 제안 작품 소개

## 구현 및 결과분석

## 결론 및 소감

처음 작품을 시작할 때는 아무것도 아는 게 없었다. 인공지능, 기계학습, 그리고 심층학습의 차이도 명확하게 구분하지 못했고, 프로그래밍 언어도 C, C++밖에 사용하지 못했다. 무작정 BWAPI를 뜯어보며 구조가 어떻게 되어있는지, 게임 인공지능이 무슨 의미인지 조금씩 알아갔다. 교수님이 PySC2를 주셨을 때는 눈앞이 막막했다. 지금껏 써보지 않은 파이썬을 사용해야 했고, 프로젝트의 규모도 일일이 확인할 수 있는 수준이 아니었다. 그뿐 아니라 수학적 요구치도 높았다. BWAPI를 작업할 때는 여러 가능한 전략의 집합 중 최적 전략을 뽑아내는 단순한 경로 탐색 문제였다면, 강화학습은 처음 모델 설계부터 수학적, 이론적 기반이 필요했다.

 처음엔 도저히 할 수 없는 일이라는 생각이 들었다. 차근차근 수학을 공부하고, 이해가 되지 않는 과목들은 재수강을 해가며 내실을 다졌어야 했는데, 그러지 못한 자신이 부끄러웠다. 도대체 어떻게 어디서부터 시작해야할까 고민하며 졸업 작품을 멀리했다. 그렇게 기약 없는 시간을 보내던 무렵, 친구들이 모인 자리에서 졸업 작품 이야기를 털어놓았다. 그러자 한 친구가 관심을 보였다. 자신도 그 주제에 흥미가 있는데, 같이 하는 게 어떠하냐는 제안이었다. 믿을 수 없는 행운이었다. 심지어 원래 작품을 진행하고 있었고, 나랑 같이 하게 되면 한 학기가 밀리는데도 선뜻 함께해주었다.

 같이하는 사람이 있다는 것은, 지친 순간에 의지할 수 있는 사람이 있다는 것이다. 괜찮다고 말해주고, 같이 열심히 하자고 말해주는 친구가 있으니 뭐든 할 수 있겠다는 생각이 들었다. 마음을 다잡고 처음부터 시작했다. 복수전공생인 나는 수리적 기초가 부족했기 때문에 수학부터 다시보기 시작했다. 수학의 정석을 다시 폈고, 인공지능을 위한 수학책을 사서 읽었다. 개론서도 열심히 사서 읽었다. 교수님이 추천해주신 밑바닥부터 시작하는 딥러닝으로 시작해서, 핸즈온 머신러닝, 러닝 텐서플로, 그리고 파이썬과 케라스로 배우는 강화학습등 시중의 유명한 서적을 찾아 읽어보았다. 평생 쳐다도 안봤던 인터넷 강의자료와 MOOC도 끙끙대며 들었다.

 그럼에도 불구하고 인공지능의 벽은 높았다. 만약 지금처럼 좋은 라이브러리와 프레임워크들, 그리고 친구가 있지 않았다면 아마 불가능했을 것이다. 다행히 텐서플로우와 케라스의 힘은 대단했다. 수천 줄을 짜야했을 그래프와 계층 설계를 간단한 함수 형태로 지원해주니 너무 편리했다. 물론 마냥 좋았던 것은 아니다. 실수투성이였고, 도대체 왜 안되는지 이해할 수 없는 상황도 많았다. 그럴 때마다 깃헙과 스택오버플로우 등 커뮤니티, 그리고 각 프레임워크들의 공식 문서가 큰 도움이 되었다. 문서화의 중요성을 실감했고, 이미 다른 사람들이 이뤄놓은 성과를 학습하는 것이 스스로의 창발성에도 얼마나 큰 도움이 되는지 몸으로 깨달았다. 그 과정 속에서 스스로의 개발 및 연구 과정을 문서화하고, 또 남이 그렇게 정리한 문서를 읽는 습관이 생겼다.

 친구와 꾸준히 같이 작업하면서, 의사소통의 중요성을 뼈저리게 실감했다. 각자가 사정이 있고 투자할 수 있는 시간대가 달랐기 때문에 온라인으로 작업을 많이 했다. 이 과정 속에서 의견의 충돌이 있을 때도 있었고, 서로가 다른 환경에서 작업을 해서 서로의 코드를 각자 컴퓨터에서 실행하지 못하는 경우도 있었다. 이러한 경험을 하면서 가상 환경의 유용함, 형상 관리 시스템의 중요성, 그리고 매뉴얼의 중요성을 체득했다. 그 결과 지금은 서로의 문제에 대해서도 대강 짐작할 수 있게 되었고, 또 어떤 플랫폼에서든지 동일한 작업물을 낼 수 있을 정도의 추상화 능력을 키웠다.

 졸업 작품 전반을 통해 얻은 것이 정말 많다. 지금까지 안일하게 해온 공부에 대해 정신 차리는 계기가 되었으며, 문제가 생겼을 때 그것을 해결하기 위해 여러 가지 방안을 궁리하고 시도해볼 수 있는 힘이 생겼다. 전혀 모르는 일에 대해서도 공부하고 해보면 된다는 마음이 생겼고, 동료와 함께 작업하는 과정에 대해서도 알게 되었다. 그리고 다른 모든 걸 떠나서, 공부 자체가 즐거워졌다. 4학년이 되도록 한 번도 생각해본 일 없는 대학원에 대해서도 생각해보게 되었다. 앞으로 내가 어떤 삶을 살아가게 될지는 모르겠지만, 졸업작품이 삶에 있어서 큰 분기였다는 점만은 확실하다.

## 참고문헌

[1] 밑바닥부터 시작하는 딥러닝
[2] 파이썬과 케라스로 배우는 강화학습
[3] 러닝 텐서플로
[4] 처음 시작하는 파이썬
[5] 핸즈온 머신러닝
[6]

## 소스코드

최상위 경로의 Mario_A3C_DQN.py를 참조하라.